%!TEX root = paper.tex
\section{Materials and methods}
\input{data}

\subsection{Feature selection}

\indent{\indent Features of the images define their content. We recognize the information images provide us in consideration of a great number of features. Then we answer, what do we see exactly. The same process can be projected on the image classification task: image features let the classifier propose the output decision. Another advantage of the approach is that it reduces feature space for a machine learning algorithm. We often need only a piece of the information image provides, hence we do not need to process and evaluate all the pixels, which can cause additional computational expenses.}

\indent{ Selecting features is a complicated and convoluted research area itself, the statement is supported by the variety of feature types, and the need of presenting essential properties on an equal basis with the previous assertion.}

\indent{ As discussed before, we need to define the set of features describing the dataset in the best way. Supposed features must satisfy the following criterion:}

\begin{itemize}
    \item The feature space should be low-dimensional
    \item The features should correlate or correlate as little as possible
    \item Selected features should represent the content of an image as fully as possible
\end{itemize}

\indent{ We are going to group selected features and define them.}

\subsection{Color features}

\indent{\indent Overviewing the dataset, we can notice that all the plant species are mostly green. Additionally, images were recorded under specific conditions. We will use RGB color model, which stands for red, green, and blue colors, and calculate features described below.}

\begin{figure}[h]
    \centering
    \includegraphics[height=5.5cm, width=10cm]{to_rgb_sample_1}
    \caption{RGB transformation}
    \label{fig:2}
\end{figure}

\indent{Let $\{x^{(k)}\}_{i=1}^N$, where $k = 1, 2, 3 $ –– an index of a channel in RGB color space respectively, $N$ –– total number of the image pixels, $x^{(k)}_i$ –– $i$-th pixel of the $k$-th channel. We will compute sample mean and standard deviation for each channel:}

\begin{equation}
	\label{eq:1}
	\overline{x^{(k)}} = \frac{1}{N}\sum_{i=1}^{N}x^{(k)}_i 
\end{equation}

\begin{equation}
	\label{eq:2}
	 s^{(k)} = \sqrt{\frac{1}{N}\sum_{i=1}^{N}(x^{(k)}_i - \overline{x^{(k)}})^2} 
\end{equation}

\subsection{Shape features}

\indent{\indent The one widely-used approach to retrieve shape features is to detect and analyze bounding contours. We will use the boundary tracing algorithm for the boundary extraction. The designated algorithm \cite{cv1985contours} was implemented in the OpenCV \cite{opencv2000python} library for the Python programming language. The studies did not take into account contours bounding areas below a certain threshold, which was empirically chosen.

\indent{ Let $K$ be the number of detected bounding contours above the threshold in the further contour-related characteristics.}

\vspace{1cm}

\textbf{Total perimeter}. In this feature, we will count the sum of perimeters of all the areas bounded by contours:

\begin{equation}
	\label{eq:3}
	 P = \sum_{i=1}^{K}p_i,
\end{equation}
where $p_i$ –– $i$-th perimeter

\vspace{1cm}

\textbf{Entire area}. It includes all the areas bounded by contours:

\begin{equation}
	\label{eq:4}
	 S = \sum_{i=1}^{K}s_i,
\end{equation}
where $s_i$ –– $i$-th area

\vspace{1cm}

\textbf{Maximal contour area}. We will be analyzing the contours bounding maximal areas:

\begin{equation}
	\label{eq:5}
	 S_m = \max{s_i}, \; i = 1, \dots, K,
\end{equation}
where $s_i$ –– $i$-th area

\vspace{1cm}

\textbf{Rectangularity}. One of the methods to estimate rectangularity is to plot minimum bounding rectangle. Rectangularity is the ratio of the entire obect area to the minimum bounding rectangle area. This featere represent how rectangular an object is:

\begin{equation}
	f_{rect} = \frac{S}{S_{MBR}},
	\label{eq:6}
\end{equation}
where $S$ –– entire area, $S_{MBR}$ –– minimum bounding rectangle area

\vspace{1cm}

\textbf{Circularity}. Another title of this shape factor is the isoperimetric quotient, and it shows how much area pe perimeter is bounding:

\begin{equation}
	f_{circ} = \frac{4 \pi A}{P^2},
	\label{eq:7}
\end{equation}
where $P$ –– entire perimeter, $A$ –– entire area of all detected elements of a plant

\indent{ Consider the correlation matrix of the described features:}

\begin{figure}[h]
	\centering
	\includegraphics[width=12.cm, height=11.5cm]{corr_heatmap_1}
	\caption{Feature correlation matrix}
	\label{fig_corr_matrix}
\end{figure}

\indent{ Based on the data in the figure \ref{fig_corr_matrix}, we conclude that the most linearly dependent features are the entire area and the largest area, but this is not true for all classes due to the predominance of plants, bounded by the only one contour, therefore the consideration of the maximum area feature is not rejected.}

\subsection{Classification}
\indent{\indent The main method for this task is Support Vector Machine (SVM) –– a binary classification algorithm based on building a separating hyperplane. The algorithm is implemented in the Scikit-learn (\cite{scikit2011python}) library for the Python programming language.}

\indent{We will use the Radial Basis Function (RBF) as the kernel function for SVM. The choice was made based on the following advantages:}

\begin{itemize}
	\item RBF usage allows building a hyperplane when the data is not linearly separable
	\item Only one parameter that affects the resulting solution needs to be tuned
	\item The values of the RBF function are in $(0, 1]$, which does not include zero or infinity
\end{itemize}

\indent{ The SVM algorithm is sensitive to non-normalized data, especially when using the RBF kernel, which is the Euclidian distance itself. In the case, when the feature values are at different intervals, a slight difference in one of them can lead to going out of range in second feature values. The solution is to map all the values into one segment. In this task, we will choose $[0, 1]$.}